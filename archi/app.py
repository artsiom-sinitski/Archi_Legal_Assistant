import logging
import streamlit as st

from openai import OpenAI

import sys
from pathlib import Path
# sys.path.append(rf"{Path(__file__).parent}\archi")
# sys.path.append(rf"{Path(__file__).parent}\archi\src")
# print(f"Path --> {Path(__file__).parent}")

from langchain_openai import ChatOpenAI

from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

import src.constants as cnst
import src.docs_processor as dp


openai_api_key = None
# llm_model: str = cnst.AI_MODELS.get("gpt_4o_mini")
llm_model: str = cnst.AI_MODELS.get("gpt_o1")

PAGE_ICON: str = "‚öñÔ∏è"
PAGE_TITLE: str = "Virtual Legal Assistant"

# LANG_EN: str = "EN"
# LANG_RU: str = "RU"dir

# ==== Controls setup =========================================

st.set_page_config(page_title=PAGE_TITLE, page_icon=PAGE_ICON)
# ==== End Controls setup =====================================

empty_opt: str = "..."
penalty_opt: str = "–ù–µ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π —Ç–æ–≤–∞—Ä"
question_opt: str = "–ü–æ–∫—É–ø–∫–∞ —Ç–æ–≤–∞—Ä–∞"
merch_return_opt: str = "–í–æ–∑–≤—Ä–∞—Ç —Ç–æ–≤–∞—Ä–∞"
# resume_opt: str = "–†–µ–∑—é–º–∏—Ä–æ–≤–∞—Ç—å"
# translate_opt: str = "–ü–µ—Ä–µ–≤–µ—Å—Ç–∏ –Ω–∞ –¥—Ä—É–≥–æ–π —è–∑—ã–∫"

with st.sidebar:
    selected_topic: tuple[str] = st.selectbox("–í—ã–±—Ä–∞–Ω–Ω–∞—è —Ç–µ–º–∞ –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–∏:",
        (empty_opt, penalty_opt, question_opt, merch_return_opt),
        label_visibility="visible",
        placeholder="..."
    ),
    header = st.columns(1)
    # ------------------------
    header_tile0 = header[0].container(height=None, border=True)
    header_tile0.image("archi/content/img/attorney3.jpg", width=230)
    # ------------------------
    # header_tile1 = header[1].container(height=None, border=False)
    header_tile0.write("üí¨ –ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ! –ú–µ–Ω—è –∑–æ–≤—É—Ç –ê—Ä—á–∏—è")
    header_tile0.write("üí¨ –Ø - –≤–∏—Ä—Ç—É–∞–ª—å–Ω—ã–π —é—Ä–∏—Å—Ç, –∫–æ—Ç–æ—Ä—ã–π –º–æ–∂–µ—Ç:")
    header_tile0.markdown("* –æ—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ –í–∞—à–∏ –≤–æ–ø—Ä–æ—Å—ã")
    header_tile0.markdown("* –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–∏—Ç—å –Ω—É–∂–Ω—ã–µ –±–ª–∞–Ω–∫–∏")
    header_tile0.markdown("* –¥–∞—Ç—å –ø–æ–Ω—è—Ç–Ω—ã–µ –ø–æ—à–∞–≥–æ–≤—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏")
    # st.divider()
# end with

# with st.sidebar
    # pass
    # openai_api_key = st.text_input("OpenAI API Key", key="chatbot_api_key", type="password")
    # "[Get an OpenAI API key](https://platform.openai.com/account/api-keys)"
    # "[View the source code](https://github.com/streamlit/llm-examples/blob/main/Chatbot.py)"
    # "[![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/streamlit/llm-examples?quickstart=1)"
# with end

                       
if empty_opt in selected_topic[0]:      # enable chat input widget only if a topic selected
    chat_prompt_widget = st.chat_input("–°–Ω–∞—á–∞–ª–∞ –≤—ã–±–µ—Ä–∏—Ç–µ —Ç–µ–º—É –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–∏...", key="DisabledChatPrompt", disabled=True)
    st.empty()
else:
    chat_prompt_widget = st.chat_input("–ü–∏—à–∏—Ç–µ —Ç—É—Ç...", key="EnabledChatPrompt", disabled=False)

    # with st.container():
    #     st.button("–ì–æ–ª–æ—Å–æ–≤–æ–π –í–≤–æ–¥")
    #     st.button("–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –ö–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—é")
    #     st.button("–ó–∞–≥—Ä—É–∑–∏—Ç—å –§–∞–π–ª...")

    if "messages" not in st.session_state:
        st.session_state["messages"] = [{"role": "assistant", "content": f'–ö–∞–∫–æ–≤–∞ –í–∞—à–∞ —Å–∏—Ç—É–∞—Ü–∏—è (–≤–æ–ø—Ä–æ—Å) –ø–æ —Ç–µ–º–µ "{selected_topic[0]}"?'}]

    for msg in st.session_state.messages:
        st.chat_message(msg["role"]).write(msg["content"])

    if user_input := chat_prompt_widget:
        try:
            openai_api_key = st.secrets.api_credentials.api_key
        except (KeyError, AttributeError) as err:
            logging.info(str(err))

        if not openai_api_key:
            st.info("Your API key is missing or invalid!")
            st.stop()

        client = OpenAI(api_key=openai_api_key)

        # match(selected_topic[0]):
        #     case penalty_opt: pmpt_template = cnst.topic_2_prompt_mapping.get("penalty_prompt")
        #     case question_opt: pmpt_template = cnst.topic_2_prompt_mapping.get("question_prompt")
        #     case _: pmpt_template = cnst.topic_2_prompt_mapping.get("question_prompt")
        if penalty_opt == selected_topic[0]:
            pmpt_template = cnst.topic_2_prompt_mapping.get("penalty_prompt")
        elif question_opt == selected_topic[0]:
            pmpt_template = cnst.topic_2_prompt_mapping.get("question_prompt")
        elif merch_return_opt == selected_topic[0]:
            pmpt_template = cnst.topic_2_prompt_mapping.get("merch_return_prompt")
        else:
            pmpt_template = cnst.topic_2_prompt_mapping.get("question_prompt")

        PROMPT = PromptTemplate(
            template=pmpt_template,
            input_variables=["context", "question"]
        )

        llm = ChatOpenAI(
            api_key=openai_api_key,
            temperature=0,
            model=llm_model
        )

        # create the chain to answer questions
        qa_chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=dp.get_vector_db_retriever(),
            return_source_documents=True,
            chain_type_kwargs={"prompt": PROMPT}
        )

        st.session_state.messages.append({"role": "user", "content": user_input})
        st.chat_message("user").write(user_input)

        # response = client.chat.completions.create(model=llm_model, messages=st.session_state.messages)
        # assistant_response = response.choices[0].message.content

        response = qa_chain(user_input)
        assistant_response = response.get("result")

        st.session_state.messages.append({"role": "assistant", "content": assistant_response})
        st.chat_message("assistant").write(assistant_response)

# if __name__ == "__main__":
#     pass
